loading config
init logging
init model
init dataset (the first time this will take a while)
init trainer
starting training

contrastive_temperature: 0.08
contrastive_lambda: 0.01

 Epoch 0:
training:
saving checkpoint
test iteration in CL trainer
[Epoch 0] total: 0.1581, rec: 0.1471, cl: 1.1093
testing:
test iteration in contrastive ranking trainer
saving scores
[Epoch CL 0] test loss: 0.1035, ndcg@5: 0.3537, ndcg@10: 0.4157, mrr: 0.3694, ctr@1: 0.1956, auc: 0.6676, acc: 0.8948, rec: 0.0391, prec: 0.0322

 Epoch 1:
training:
saving checkpoint
test iteration in CL trainer
[Epoch 1] total: 0.1519, rec: 0.1427, cl: 0.9220
testing:
test iteration in contrastive ranking trainer
saving scores
[Epoch CL 1] test loss: 0.0978, ndcg@5: 0.3538, ndcg@10: 0.4164, mrr: 0.3706, ctr@1: 0.1985, auc: 0.6693, acc: 0.8976, rec: 0.0237, prec: 0.0250

 Epoch 2:
training:
saving checkpoint
test iteration in CL trainer
[Epoch 2] total: 0.1493, rec: 0.1411, cl: 0.8195
testing:
test iteration in contrastive ranking trainer
saving scores
[Epoch CL 2] test loss: 0.1011, ndcg@5: 0.3564, ndcg@10: 0.4190, mrr: 0.3722, ctr@1: 0.1973, auc: 0.6695, acc: 0.8948, rec: 0.0416, prec: 0.0424

 Epoch 3:
training:
saving checkpoint
test iteration in CL trainer
[Epoch 3] total: 0.1478, rec: 0.1403, cl: 0.7498
testing:
test iteration in contrastive ranking trainer
saving scores
[Epoch CL 3] test loss: 0.1015, ndcg@5: 0.3616, ndcg@10: 0.4232, mrr: 0.3770, ctr@1: 0.2035, auc: 0.6763, acc: 0.8941, rec: 0.0506, prec: 0.0488

 Epoch 4:
training:
saving checkpoint
test iteration in CL trainer
[Epoch 4] total: 0.1466, rec: 0.1395, cl: 0.7088
testing:
test iteration in contrastive ranking trainer
saving scores
[Epoch CL 4] test loss: 0.1013, ndcg@5: 0.3594, ndcg@10: 0.4218, mrr: 0.3745, ctr@1: 0.1993, auc: 0.6747, acc: 0.8926, rec: 0.0528, prec: 0.0544
